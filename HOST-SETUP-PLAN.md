# ðŸš€ Complete Setup Plan: MyCoder DevContainer with Remote Access

**Created:** January 24, 2026  
**Updated:** January 24, 2026  
**Goal:** Set up the ticket-processor devcontainer on Linux with Podman + Ollama, accessible locally and remotely via Cloudflare Tunnel.

**Host Machine Reference:** See [HOST-MACHINE-REFERENCE.md](HOST-MACHINE-REFERENCE.md) for system specs and current state.

---

## Legend

| Symbol | Meaning |
|--------|---------|
| ðŸ§‘ **[JocÃ³]** | Task for you (requires physical/manual action) |
| ðŸ¤– **[LLM]** | Task for Claude (code/config changes, running commands) |
| âœ… | Completed |
| â¬œ | To do |

---

## System Optimizations Applied âœ…

These optimizations were previously configured on this machine:

| Optimization | Description | Status |
|--------------|-------------|--------|
| **8GB Swap File** | Memory safety buffer on NVMe SSD to prevent OOM crashes during VRAM-to-RAM offloading | âœ… Active |
| **ZRAM Compression** | RAM compression for more efficient memory use, reduces disk swap reliance | âœ… Active |
| **Storage Redirection** | Podman/Ollama configured to use 4TB HDD for model storage instead of 80GB root SSD | â¬œ Pending |

---

## Phase 1: Linux PC Configuration

### 1.1 Prerequisites Check

- [x] âœ… Podman installed: **v4.9.3**
- [x] âœ… NVIDIA drivers working: **RTX 3090 (24GB VRAM)**, Driver **570.211.01**
- [x] âœ… nvidia-container-toolkit installed
- [x] âœ… Ollama installed and running as systemd service
- [x] âœ… Node.js installed: **v24.13.0**
- [x] âœ… npm installed: **v11.6.2**
- [x] âœ… cloudflared installed: `/usr/local/bin/cloudflared`

### 1.2 Remaining Setup Tasks

- [x] âœ… ðŸ¤– **[LLM]** Configure Ollama to listen on all interfaces (0.0.0.0:11434)

- [x] âœ… ðŸ¤– **[LLM]** Install global npm packages (pm2 6.0.14, kodu, backlog.md)

- [x] âœ… ðŸ¤– **[LLM]** Install project dependencies (npm install)

- [ ] ðŸ§‘ **[JocÃ³]** Pull Ollama models (requires choosing which models you want)
  ```bash
  # Recommended for RTX 3090 (24GB VRAM):
  ollama pull qwen2.5-coder:32b    # ~20GB, excellent for coding
  ollama pull deepseek-coder:6.7b  # ~4GB, fast for simple tasks
  ```

### 1.3 Firewall Status

- [x] âœ… ðŸ¤– **[LLM]** Checked UFW status - **INACTIVE** (no firewall blocking)

### 1.4 Storage Redirection âœ…

- [x] âœ… ðŸ¤– **[LLM]** Mounted 4TB HDD at `/mnt/hdd` (NTFS, preserving data)
- [x] âœ… ðŸ¤– **[LLM]** Added to `/etc/fstab` for auto-mount on boot
- [x] âœ… ðŸ¤– **[LLM]** Created `/mnt/hdd/llm-data/ollama` for model storage
- [x] âœ… ðŸ¤– **[LLM]** Created `/mnt/hdd/llm-data/podman` for container images
- [x] âœ… ðŸ¤– **[LLM]** Configured Ollama: `OLLAMA_MODELS=/mnt/hdd/llm-data/ollama`
- [x] âœ… ðŸ¤– **[LLM]** Configured Podman: `graphroot=/mnt/hdd/llm-data/podman/storage`

---

## Phase 2: DevContainer Configuration âœ…

### 2.1 DevContainer Files Updated

- [x] âœ… ðŸ¤– **[LLM]** Updated `.devcontainer/devcontainer.json`:
  - Podman compatibility (`--userns=keep-id`, `--security-opt=label=disable`)
  - Uses `host.containers.internal` for Ollama access
  - Added useful VS Code extensions
  
- [x] âœ… ðŸ¤– **[LLM]** Updated `.devcontainer/Dockerfile`:
  - Node.js 24 base image
  - Added jq, vim for debugging
  
- [x] âœ… ðŸ¤– **[LLM]** Updated `.devcontainer/setup-tools.sh`:
  - Installs pm2, kodu, backlog.md
  - Tests Ollama connectivity
  - Shows quick start commands

- [ ] ðŸ¤– **[LLM]** Update `containers/Dockerfile` to include:
  - GPU-enabled base image option
  - Kilo Code CLI configuration
  - Development tools

- [ ] ðŸ¤– **[LLM]** Update `containers/podman-compose.yml` for:
  - GPU passthrough to containers
  - Proper network configuration for external access
  - Volume mounts for persistence

### 2.2 Environment Configuration

- [x] âœ… ðŸ¤– **[LLM]** `.env.example` already exists with all required variables

- [ ] ðŸ§‘ **[JocÃ³]** Create `.env` file and configure (when ready to start containers)
  ```bash
  cd ~/dev/mycoder
  cp .env.example .env
  nano .env   # Edit with your values
  ```

### 2.3 Test DevContainer

- [ ] ðŸ¤– **[LLM]** Build and test devcontainer
- [ ] ðŸ§‘ **[JocÃ³]** Open in VS Code and "Reopen in Container"

### 2.4 Start Full Stack (Optional - for Gitea integration)

- [ ] ðŸ§‘ **[JocÃ³]** Start containers with Podman Compose (when ready)
  ```bash
  cd ~/dev/mycoder/containers
  podman-compose up -d
  podman ps   # Should show: gitea, postgres, ticket-processor-app
  ```

---

## Phase 3: Local Network Access (From Laptop)

### 3.1 Find Your Server's IP

- [ ] ðŸ§‘ **[JocÃ³]** Get the Linux PC's local IP address
  ```bash
  ip addr show | grep "inet " | grep -v 127.0.0.1
  # Note the IP, e.g., 192.168.1.100
  ```

### 3.2 Test Local Access

From your laptop on the same network:

- [ ] ðŸ§‘ **[JocÃ³]** Test Ollama access
  ```bash
  curl http://<SERVER-IP>:11434/api/tags
  ```

- [ ] ðŸ§‘ **[JocÃ³]** Test Gitea access
  - Open browser: `http://<SERVER-IP>:3000`

- [ ] ðŸ§‘ **[JocÃ³]** Test webhook server
  ```bash
  curl http://<SERVER-IP>:3001/health
  ```

### 3.3 VS Code Remote Development Options

**Option A: SSH Remote (Recommended)**

- [ ] ðŸ§‘ **[JocÃ³]** Enable SSH on the Linux PC
  ```bash
  sudo apt install openssh-server
  sudo systemctl enable ssh
  sudo systemctl start ssh
  ```

- [ ] ðŸ§‘ **[JocÃ³]** From laptop VS Code:
  1. Install "Remote - SSH" extension
  2. Connect to `ssh user@<SERVER-IP>`
  3. Open folder `/home/mandulaj/dev/mycoder`
  4. Reopen in Container (uses Podman)

**Option B: code-server (Web-based VS Code)**

- [ ] ðŸ§‘ **[JocÃ³]** Install code-server
  ```bash
  curl -fsSL https://code-server.dev/install.sh | sh
  ```

- [ ] ðŸ§‘ **[JocÃ³]** Configure code-server
  ```bash
  mkdir -p ~/.config/code-server
  cat > ~/.config/code-server/config.yaml << EOF
  bind-addr: 0.0.0.0:8080
  auth: password
  password: <your-secure-password>
  cert: false
  EOF
  ```

- [ ] ðŸ§‘ **[JocÃ³]** Start code-server
  ```bash
  sudo systemctl enable --now code-server@$USER
  ```

- [ ] ðŸ§‘ **[JocÃ³]** Access from laptop browser: `http://<SERVER-IP>:8080`

---

## Phase 4: Remote Access via Cloudflare Tunnel

### 4.1 Current Setup

You already have **cloudflared running on your Synology NAS** (192.168.0.5), which is the recommended approach:

âœ… **Advantages of NAS-hosted tunnel:**
- NAS is always on (unlike dev PC which may sleep/shutdown)
- Centralized tunnel management for all home services
- Dev server doesn't need to run cloudflared
- Can add/remove services via NAS configuration

### 4.2 Network Topology

```
Internet â†’ Cloudflare â†’ Synology NAS (192.168.0.5) â†’ Dev Server (192.168.0.10)
                         cloudflared tunnel              :11434 Ollama
                                                         :3000 Gitea
                                                         :3001 Webhook
                                                         :8080 code-server
```

### 4.3 Configure Tunnel Routes on Synology NAS

- [ ] ðŸ§‘ **[JocÃ³]** Add routes to your existing Cloudflare tunnel on the NAS

  In the Synology Cloudflare tunnel config (or Cloudflare Zero Trust dashboard), add these public hostnames pointing to the dev server:

  | Public Hostname | Service URL | Description |
  |-----------------|-------------|-------------|
  | `code.yourdomain.com` | `http://192.168.0.10:8080` | VS Code Server |
  | `git.yourdomain.com` | `http://192.168.0.10:3000` | Gitea |
  | `ollama.yourdomain.com` | `http://192.168.0.10:11434` | Ollama API |
  | `webhook.yourdomain.com` | `http://192.168.0.10:3001` | Webhook endpoint |

  **Via Cloudflare Dashboard:**
  1. Go to: Zero Trust â†’ Networks â†’ Tunnels
  2. Select your existing tunnel
  3. Add Public Hostname for each service
  4. Set Service to the dev server IP:port

### 4.4 Security Considerations

- [ ] ðŸ§‘ **[JocÃ³]** Add Cloudflare Access policies (recommended for Ollama)
  - Go to: Cloudflare Dashboard â†’ Zero Trust â†’ Access â†’ Applications
  - Create policies requiring authentication for sensitive endpoints
  - **Especially important for Ollama** - don't expose LLM API publicly without auth!
  - Options: Email OTP, GitHub, Google auth

### 4.5 Alternative: Run Tunnel on Dev Server (Backup)

If you prefer to run the tunnel directly on the dev server (cloudflared is already installed):

```bash
# Authenticate (one-time)
cloudflared tunnel login

# Create tunnel
cloudflared tunnel create mycoder-dev

# Configure and run as service
sudo cloudflared service install
sudo systemctl enable --now cloudflared
```

---

## Phase 5: Final Verification Checklist

### Local (on Linux PC)

- [ ] ðŸ§‘ **[JocÃ³]** `ollama list` shows your models
- [ ] ðŸ§‘ **[JocÃ³]** `podman ps` shows all containers running
- [ ] ðŸ§‘ **[JocÃ³]** `curl http://localhost:3000` returns Gitea page
- [ ] ðŸ§‘ **[JocÃ³]** `curl http://localhost:3001/health` returns OK
- [ ] ðŸ§‘ **[JocÃ³]** `curl http://localhost:11434/api/tags` returns models

### Local Network (from laptop)

- [ ] ðŸ§‘ **[JocÃ³]** Can access `http://<SERVER-IP>:8080` (code-server) OR connect via SSH
- [ ] ðŸ§‘ **[JocÃ³]** Can access `http://<SERVER-IP>:3000` (Gitea)
- [ ] ðŸ§‘ **[JocÃ³]** Can run Kilo Code with Ollama from devcontainer

### Remote (via Cloudflare)

- [ ] ðŸ§‘ **[JocÃ³]** Can access `https://code.yourdomain.com`
- [ ] ðŸ§‘ **[JocÃ³]** Can access `https://git.yourdomain.com`
- [ ] ðŸ§‘ **[JocÃ³]** Cloudflare Access policies work (if configured)

### Test Full Workflow

- [ ] ðŸ§‘ **[JocÃ³]** Create a test task
  ```bash
  node scripts/create-task.js
  ```
- [ ] ðŸ§‘ **[JocÃ³]** Watch it process
  ```bash
  podman-compose logs -f ticket-processor-app
  ```
- [ ] ðŸ§‘ **[JocÃ³]** Verify PR created in Gitea

---

## Quick Reference: Service Management

```bash
# Ollama
sudo systemctl status ollama
sudo systemctl restart ollama

# Podman containers
cd ~/dev/mycoder/containers
podman-compose ps
podman-compose up -d
podman-compose down
podman-compose logs -f

# Cloudflare Tunnel
sudo systemctl status cloudflared
sudo systemctl restart cloudflared

# code-server
sudo systemctl status code-server@$USER
sudo systemctl restart code-server@$USER
```

---

## Next Steps After Setup

1. ðŸ§‘ **[JocÃ³]** Start with Phase 1, tick off each item
2. ðŸ¤– **[LLM]** I'll update the devcontainer configs when you're ready for Phase 2
3. ðŸ§‘ **[JocÃ³]** Test local network access (Phase 3)
4. ðŸ§‘ **[JocÃ³]** Set up Cloudflare (Phase 4) when you need remote access

---

**Let me know when you're ready to proceed with any phase, and I'll help with the LLM tasks!**
